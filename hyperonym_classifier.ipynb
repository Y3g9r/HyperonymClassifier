{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hyperonym_classifier.ipynb","provenance":[],"collapsed_sections":["7_eR-sfLmZqv","pwVi5QcdS032"],"mount_file_id":"1DkCYh2E0m17MzsPsHtvnSFSZ1h5cqnYf","authorship_tag":"ABX9TyNqCgAgKxfNx4yIMk3KweGR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Классификация слов по контексту"],"metadata":{"id":"Gipzjp0emGeB"}},{"cell_type":"markdown","source":["##Подгрузка импортов"],"metadata":{"id":"7_eR-sfLmZqv"}},{"cell_type":"code","source":["pip install transformers"],"metadata":{"id":"TQBvUdACv6cH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648982982595,"user_tz":-180,"elapsed":5632,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}},"outputId":"b6de47d0-ef6e-4f28-c42d-facd88b037d1"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch\n","import transformers\n","import json,io\n","from transformers import BertTokenizerFast, BertModel, Trainer, TrainingArguments"],"metadata":{"id":"lX9tDuGq7sXh","executionInfo":{"status":"ok","timestamp":1648982982596,"user_tz":-180,"elapsed":19,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}}},"execution_count":101,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AafmW9sxo5zf","executionInfo":{"status":"ok","timestamp":1648982987044,"user_tz":-180,"elapsed":4465,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}},"outputId":"17f0e27f-81ee-4405-c104-d6fb2e04e197"},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")"],"metadata":{"id":"PknBVt67HfPZ","executionInfo":{"status":"ok","timestamp":1648982987044,"user_tz":-180,"elapsed":12,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}}},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":["## Подготовка датасета"],"metadata":{"id":"w6zdFV5tNMlg"}},{"cell_type":"code","source":["#максимальная длинна предложения\n","MAX_LENGTH = 120"],"metadata":{"id":"QQNcLCPbDIFl","executionInfo":{"status":"ok","timestamp":1648984958110,"user_tz":-180,"elapsed":3,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}}},"execution_count":121,"outputs":[]},{"cell_type":"code","source":["def generate_bert_input(file_name,model_name):\n","    ###\n","    #input data:\n","    #\n","    #file_name: имя файла формата json c сэмплами вида [[[5, 10]], [\"Если монах ведёт себя согласно со своими обетами, он не страшилище, не чуждое, почти враждебное нам существо, не живой труп, а наоборот, великий духовный друг наш и отец, носитель духовной благодатной жизни, молитвенник за нас пред Богом.\"], [\"монах 0\"], [\"член религиозной общины, давший обет ведения аскетической жизни\"], [1]]\n","    #model_name: имя bert модели\n","    #\n","    #return:\n","    #\n","    #лист картежей вида ((tokens_tensor, segments_tensors, offset_mapping, samp_position),(tokens_tensor, segments_tensors, offset_mapping, samp_position),label)\n","    #где tokens_tensor токенезированные слова определения или примера употребления в зависимости от картежа соответственно\n","    #где segments_tensors длина  определения или примера употребления в зависимости от картежа соответственно\n","    #где offset_mapping позиция ключеовго слова  определения(всегда первое) или примера употребления в зависимости от картежа соответственно\n","    #где samp_position позиция ключеовго слова из базы данных для определения(всегда первое) или примера употребления в зависимости от картежа соответственно\n","    ###\n","    data = []\n","    with io.open(file_name, 'r', encoding=\"utf-8-sig\") as f:\n","        data = json.load(f)\n","    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n","    samples_data = []\n","    i = 0\n","    for text in data:\n","        if i==800:\n","          break\n","        #получаем данные для примера употребления\n","        marked_text = text[1][0]\n","        tokenized_text = tokenizer(marked_text, return_offsets_mapping=True,max_length=MAX_LENGTH,padding='max_length',truncation=True)\n","        offset_mapping = tokenized_text['offset_mapping']\n","        indexed_tokens = tokenized_text['input_ids']\n","        segments_ids = [1] * len(tokenized_text['input_ids'])\n","        tokens_tensor = torch.tensor([indexed_tokens])\n","        segments_tensors = torch.tensor([segments_ids])\n","        samp_position = text[0][0]\n","        sample_data = (tokens_tensor, segments_tensors, offset_mapping, samp_position)\n","\n","        #получаем данные для определения\n","        #берём само слово text[2][0][:-2] и его определение text[3][0] и строим текст вида\n","        #(слово - определение)\n","        definition = text[2][0][:-2]+\" - \"+text[3][0]\n","        def_positions = [0, len(text[2][0][:-2])]\n","\n","        marked_text = definition \n","        tokenized_text = tokenizer(marked_text, return_offsets_mapping=True,max_length=MAX_LENGTH,padding='max_length',truncation=True)\n","        offset_mapping = tokenized_text['offset_mapping']\n","        indexed_tokens = tokenized_text['input_ids']\n","        segments_ids = [1] * len(tokenized_text['input_ids'])\n","        tokens_tensor = torch.tensor([indexed_tokens])\n","        segments_tensors = torch.tensor([segments_ids])\n","        def_data = (tokens_tensor, segments_tensors, offset_mapping, def_positions)\n","\n","        temp_data = ((def_data),(sample_data),text[4])\n","        samples_data.append(temp_data)\n","        i += 1\n","    return samples_data"],"metadata":{"id":"OTKb5XIh8vL2","executionInfo":{"status":"ok","timestamp":1648984959503,"user_tz":-180,"elapsed":3,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}}},"execution_count":122,"outputs":[]},{"cell_type":"code","source":["class HyperonymDataset(torch.utils.data.Dataset):\n","  \n","  def __init__(self, dataset):\n","      self.dataset = dataset\n","\n","  def __len__(self):\n","        return len(self.dataset)\n","\n","  def __getitem__(self, idx):\n","        items = {\n","        \"tokens_tensor_def\": torch.tensor(self.dataset[idx][0][0]),\n","        \"segments_tensors_def\": torch.tensor(self.dataset[idx][0][1]),\n","        \"offset_mapping_def\": torch.tensor(self.dataset[idx][0][2]),\n","        \"samp_position_def\": torch.tensor(self.dataset[idx][0][3]),\n","\n","        \"tokens_tensor_samp\": torch.tensor(self.dataset[idx][1][0]),\n","        \"segments_tensors_samp\": torch.tensor(self.dataset[idx][1][1]),\n","        \"offset_mapping_samp\": torch.tensor(self.dataset[idx][1][2]),\n","        \"samp_position_samp\": torch.tensor(self.dataset[idx][1][3]),\n","\n","        \"labels\": torch.tensor(self.dataset[idx][2][0])\n","        }\n","\n","        return items\n","        # items = {\"dataset\": self.dataset[idx]}\n","        # return items"],"metadata":{"id":"Gb-wPNHKrMF1","executionInfo":{"status":"ok","timestamp":1648984962243,"user_tz":-180,"elapsed":402,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}}},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":["##Модель"],"metadata":{"id":"y7TqUIKXEytM"}},{"cell_type":"code","source":["class SemanticSimilarityBertModel(torch.nn.Module):\n","    def __init__(self):\n","        super(SemanticSimilarityBertModel, self).__init__()\n","\n","        #Подгружаем модель\n","        model_name = 'sberbank-ai/ruBert-base'\n","        self.model = BertModel.from_pretrained(model_name, output_hidden_states = True)\n","\n","        #Файн-тьюниги\n","        self.biderectional_lstm = torch.nn.LSTM(input_size = 768*2, hidden_size = 768, bidirectional =True, batch_first = True)\n","       # self.AvgPool1D = torch.nn.AvgPool1d(768*2) \n","        #self.MaxPool1D = torch.nn.MaxPool1d(768*2)\n","        #self.concatenate = torch.cat((self.AvgPool1D,self.MaxPool1D),dim = 1)\n","        self.Dropout = torch.nn.Dropout(0.3)\n","        self.Linear =  torch.nn.Linear(3072, 2)\n","\n","    def forward(self, tokens_tensor_def,segments_tensors_def,offset_mapping_def,samp_position_def, tokens_tensor_samp,segments_tensors_samp,offset_mapping_samp,samp_position_samp, labels):\n","        # ex = [[[5, 10]], [\"Если монах ведёт себя согласно со своими обетами, он не страшилище, не чуждое, почти враждебное нам существо, не живой труп, а наоборот, великий духовный друг наш и отец, носитель духовной благодатной жизни, молитвенник за нас пред Богом.\"], [\"монах 0\"], [\"член религиозной общины, давший обет ведения аскетической жизни\"], [1]]\n","\n","        #создаим пустой тезнор размерности 1x2x1 для хранения эмбедингов сэмплов из батчей (итоговый будет размерности Nx2x1536,где N - число сэмплов в батче)\n","        embd_batch = torch.tensor([[[],[]]]).to(device)\n","        #необходимо для правильной установки размерности в батче embd_batch эмбедингов\n","        first_pass = False\n","        for i in range(len(labels)):\n","            #получаем эмбединги ключевого слова из примера употребления\n","            example_token_vec = self.get_vector(tokens_tensor_samp[i],segments_tensors_samp[i])\n","            examples_token_key_word_position = self.token_detection(offset_mapping_samp[i],samp_position_samp[i])\n","            example_embeddings = self.vector_recognition(example_token_vec, examples_token_key_word_position)\n","\n","            #получаем эмбединги ключевого слова из определения\n","            def_token_vec = self.get_vector(tokens_tensor_def[i],segments_tensors_def[i])\n","            def_token_key_word_position = self.token_detection(offset_mapping_def[i],samp_position_def[i])\n","            def_embeddings = self.vector_recognition(def_token_vec, def_token_key_word_position)\n","\n","            #объединяем два вектора в 1 и добавляем в общий массив (получаем тензор 2x1536)\n","            embd_sample = torch.stack((example_embeddings,def_embeddings)).to(device)\n","            if not first_pass:\n","              embd_batch = torch.cat((embd_batch,embd_sample.unsqueeze(0)),-1)\n","              first_pass = True\n","            else:\n","              embd_batch = torch.cat((embd_batch,embd_sample.unsqueeze(0)),0)\n","\n","        # print(f\"embd shape {embd_batch.shape}\")\n","        x,_ = self.biderectional_lstm(embd_batch)\n","        # print(f\"x shape {x.shape}\")\n","        #GlobalAveragePooling1D заменяется торч мином по 1 измерению mean(dim=(1))\n","        Ax = torch.mean(x,1)\n","        #MaxAveragePooling1D заменяется  Mx,_ = torch.max(x,1)\n","        Mx,_ = torch.max(x,1)\n","        # print(f\"ax shape {Ax.shape} Mx shape {Mx.shape}\")\n","        concatenate = torch.cat((Ax, Mx), dim=1)\n","        # print(f\"conc shape {concatenate.shape}\")\n","        Do = self.Dropout(concatenate)\n","        logits = self.Linear(Do)\n","        loss = None\n","\n","        if labels is not None:\n","            loss_fct = torch.nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n","\n","        #transformers.modeling_outputs.SequenceClassifierOutput используется как для обучения так\n","        #и для функции compute_metrics\n","        return transformers.modeling_outputs.SequenceClassifierOutput(\n","            logits=logits,\n","            loss=loss,\n","        )\n","\n","\n","    def token_detection(self, token_map, position):\n","        #Функция определения ключевого слова\n","        \"\"\"\n","        :param token_map: list of tuples of begin and end of every token\n","        :param position:  list of type: [int,int]\n","        :return: list of key word tokens position\n","        \"\"\"\n","        #из за того что в начале стоит CLS позиции начала и конца ключевого слова сдвигаются на 5\n","        begin_postion = position[0] #+ 5\n","        end_position = position[1] #+ 5\n","\n","        position_of_key_tokens = []\n","        for token_tuple in range(1,len(token_map)-1):\n","            #if token is one\n","            if token_map[token_tuple][0] == begin_postion and token_map[token_tuple][1] == end_position:\n","                position_of_key_tokens.append(token_tuple)\n","                break\n","\n","            #if we have multipli count of tokens for one key word\n","            if token_map[token_tuple][0] >= begin_postion and token_map[token_tuple][1] != end_position:\n","                position_of_key_tokens.append(token_tuple)\n","            if token_map[token_tuple][0] != begin_postion and token_map[token_tuple][1] == end_position:\n","                position_of_key_tokens.append(token_tuple)\n","                break\n","\n","        return position_of_key_tokens\n","    \n","    def get_vector(self, tokens_tensor, segments_tensors):\n","        #Функция получения вектора ключевого слова\n","        with torch.no_grad():\n","            outputs = self.model(tokens_tensor, segments_tensors)\n","            hidden_states = outputs[2]\n","        #from [# layers, # batches, # tokens, # features] to [# tokens, # layers, # features]\n","        token_dim = torch.stack(hidden_states, dim=0)\n","        token_dim = torch.squeeze(token_dim, dim=1)\n","        token_dim = token_dim.permute(1, 0, 2)\n","        token_vecs_cat = []\n","        for token in token_dim:\n","            cat_vec = torch.cat((token[-1], token[-2]), dim=0)\n","            token_vecs_cat.append(cat_vec)\n","\n","        return token_vecs_cat\n","\n","    def get_avarage_embedding(self,embeddings_list, positions_list):\n","        #Функция получения среднего вектора\n","        avg_tensor = torch.stack((embeddings_list[positions_list[0]],))\n","        for i in range(1, len(positions_list)):\n","              avg_tensor = torch.cat((avg_tensor,embeddings_list[positions_list[i]].unsqueeze(0)))\n","\n","        average_embedding = torch.mean(avg_tensor,0)\n","        return average_embedding \n","\n","    def vector_recognition(self, tokens_embeddings_ex, tokens_key_word_position_ex):\n","        #Функция подготовки вектора в зависимости от количества токенов,которым представляется ключевое слово\n","        if len(tokens_key_word_position_ex) > 1:\n","            embeddings_data = torch.tensor(self.get_avarage_embedding(tokens_embeddings_ex,tokens_key_word_position_ex))\n","        else:\n","            # print(tokens_embeddings_ex)\n","            # print(tokens_key_word_position_ex)\n","            embeddings_data = torch.tensor(tokens_embeddings_ex[tokens_key_word_position_ex[0]])\n","        return embeddings_data\n","\n"],"metadata":{"id":"cw_cP3AhE2Pq","executionInfo":{"status":"ok","timestamp":1648984976294,"user_tz":-180,"elapsed":827,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}}},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":["##Разбиение обучающей и тестовой выборок"],"metadata":{"id":"4Wvyv4X7Csb9"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"kHNIw6egTYFf","executionInfo":{"status":"ok","timestamp":1648982987046,"user_tz":-180,"elapsed":6,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}}},"execution_count":108,"outputs":[]},{"cell_type":"code","source":["data = generate_bert_input('/content/drive/MyDrive/dataset_not_embeddings.json','sberbank-ai/ruBert-base')"],"metadata":{"id":"2IrZ2HtgC1he","executionInfo":{"status":"ok","timestamp":1648984985282,"user_tz":-180,"elapsed":5642,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b312ac09-0019-43d4-e333-f7f42496ad66"},"execution_count":125,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file https://huggingface.co/sberbank-ai/ruBert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/894ee6c75173bea620f08bc907dbb99c6da1a0cee7ce7f81c3ad91dbb0d3b2a6.8a15a3f9a238139bbd1d85ed0715ebad4078d4919f57be2a94d999cb6ff4d054\n","loading file https://huggingface.co/sberbank-ai/ruBert-base/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/sberbank-ai/ruBert-base/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/sberbank-ai/ruBert-base/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/sberbank-ai/ruBert-base/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/sberbank-ai/ruBert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3ff2b30ffd2e83991ada1f23ca4d7adad284baa741ea21704f02d83b72405c79.072018299fd210a7d734e8e87cbe1d148c2d27f90ed55251c1d9472dc018ce32\n","loading configuration file https://huggingface.co/sberbank-ai/ruBert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3ff2b30ffd2e83991ada1f23ca4d7adad284baa741ea21704f02d83b72405c79.072018299fd210a7d734e8e87cbe1d148c2d27f90ed55251c1d9472dc018ce32\n"]}]},{"cell_type":"code","source":["RANDOM_SEED=1\n","data_train, data_test = train_test_split(data, test_size=0.4, random_state=RANDOM_SEED)\n","data_val, data_test = train_test_split(data_test, test_size=0.5, random_state=RANDOM_SEED)"],"metadata":{"id":"AlD1o5UtyrkN","executionInfo":{"status":"ok","timestamp":1648984993030,"user_tz":-180,"elapsed":294,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}}},"execution_count":127,"outputs":[]},{"cell_type":"code","source":["hd_train = HyperonymDataset(data_train)\n","hd_val = HyperonymDataset(data_val)\n","hd_test = HyperonymDataset(data_test)"],"metadata":{"id":"Hk3m8W7whvpd","executionInfo":{"status":"ok","timestamp":1648984993889,"user_tz":-180,"elapsed":470,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}}},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":["##Задание параметров обучения"],"metadata":{"id":"pwVi5QcdS032"}},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=2,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=16,\n","    warmup_steps=0,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    optim = \"adamw_hf\"\n",")"],"metadata":{"id":"aYlEOmYAS7nf","executionInfo":{"status":"ok","timestamp":1648982992784,"user_tz":-180,"elapsed":6,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0bfbc2c-cc5d-4b44-839e-549a4ba0f992"},"execution_count":112,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","\n","def compute_metrics(pred):\n","    predictions, labels = pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=labels)"],"metadata":{"id":"Thrp9iXJS9PO","executionInfo":{"status":"ok","timestamp":1648982992785,"user_tz":-180,"elapsed":6,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}}},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":["##Обучение  и оценка модели"],"metadata":{"id":"oXxHMYGT6GLO"}},{"cell_type":"code","source":["model = SemanticSimilarityBertModel()\n","model = model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qGcw_SJ_6Lyl","executionInfo":{"status":"ok","timestamp":1648985001125,"user_tz":-180,"elapsed":4700,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}},"outputId":"77eb069e-0bad-48fa-dc6a-ae2e38ba1a87"},"execution_count":129,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/sberbank-ai/ruBert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3ff2b30ffd2e83991ada1f23ca4d7adad284baa741ea21704f02d83b72405c79.072018299fd210a7d734e8e87cbe1d148c2d27f90ed55251c1d9472dc018ce32\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 120138\n","}\n","\n","loading weights file https://huggingface.co/sberbank-ai/ruBert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/c6b6109f95adac55ac7d53cc48641223739f281fe852ef8925a98d6efa2ead30.97fbdf1d2c71997b98f1b22260e739b13c3b5b9a8b7e673620aa5986b8d892c0\n","Some weights of the model checkpoint at sberbank-ai/ruBert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertModel were initialized from the model checkpoint at sberbank-ai/ruBert-base.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"]}]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=hd_train,\n","    eval_dataset=hd_val,\n","    compute_metrics = compute_metrics\n",")\n","\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":868},"id":"T2jjFWjN8lEm","executionInfo":{"status":"ok","timestamp":1648985947778,"user_tz":-180,"elapsed":945767,"user":{"displayName":"ivan ivanov","userId":"09891931241776312952"}},"outputId":"e1092ee6-5fa3-4b79-c45d-d6c7bf1255c7"},"execution_count":130,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 480\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 120\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  app.launch_new_instance()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [120/120 15:35, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>0.560700</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.487400</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.497400</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.588000</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.541400</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.551300</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.473500</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.493100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.393700</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.457900</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.587100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=120, training_loss=0.5109731356302897, metrics={'train_runtime': 945.3962, 'train_samples_per_second': 1.015, 'train_steps_per_second': 0.127, 'total_flos': 0.0, 'train_loss': 0.5109731356302897, 'epoch': 2.0})"]},"metadata":{},"execution_count":130}]}]}